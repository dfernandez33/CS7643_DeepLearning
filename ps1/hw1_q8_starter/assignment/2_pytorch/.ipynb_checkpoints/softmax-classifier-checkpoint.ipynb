{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch data\n",
    "\n",
    "PyTorch comes with a nice paradigm for dealing with data which we'll use here. A PyTorch [`Dataset`](http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset) knows where to find data in its raw form (files on disk) and how to load individual examples into Python datastructures. A PyTorch [`DataLoader`](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) takes a dataset and offers a variety of ways to sample batches from that dataset.\n",
    "\n",
    "Take a moment to browse through the `CIFAR10` `Dataset` in `2_pytorch/cifar10.py`, read the `DataLoader` documentation linked above, and see how these are used in the section of `train.py` that loads data. Note that in the first part of the homework we subtracted a mean CIFAR10 image from every image before feeding it in to our models. Here we subtract a constant color instead. Both methods are seen in practice and work equally well.\n",
    "\n",
    "PyTorch provides lots of vision datasets which can be imported directly from [`torchvision.datasets`](http://pytorch.org/docs/master/torchvision/datasets.html). Also see [`torchtext`](https://github.com/pytorch/text#datasets) for natural language datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier in PyTorch\n",
    "\n",
    "In PyTorch Deep Learning building blocks are implemented in the neural network module [`torch.nn`](http://pytorch.org/docs/master/nn.html#) (usually imported as `nn`). A PyTorch model is typically a subclass of [`nn.Module`](http://pytorch.org/docs/master/nn.html#torch.nn.Module) and thereby gains a multitude of features. Because your logistic regressor is an `nn.Module` all of its parameters and sub-modules are accessible through the `.parameters()` and `.modules()` methods.\n",
    "\n",
    "Now implement a softmax classifier by filling in the marked sections of `models/softmax.py`. \n",
    "\n",
    "The main driver for this question is `train.py`. It reads arguments and model hyperparameter from the command line, loads CIFAR10 data and the specified model (in this case, softmax). Using the optimizer initialized with appropriate hyperparameters, it trains the model and reports performance on test data. \n",
    "\n",
    "Complete the following couple of sections in `train.py`:\n",
    "1. Initialize an optimizer from the torch.optim package\n",
    "2. Update the parameters in model using the optimizer initialized above\n",
    "\n",
    "At this point all of the components required to train the softmax classifer are complete for the softmax classifier. Now run\n",
    "\n",
    "    $ run_softmax.sh\n",
    "\n",
    "to train a model and save it to `softmax.pt`. This will also produce a `softmax.log` file which contains training details which we will visualize below.   \n",
    "\n",
    "**Note**: You may want to adjust the hyperparameters specified in `run_softmax.sh` to get reasonable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that you have completed training the classifer, let us plot the training loss vs. iteration. This is an\n",
    "# example to show a simple way to log and plot data from PyTorch.\n",
    "\n",
    "# we neeed matplotlib to plot the graphs for us!\n",
    "import matplotlib\n",
    "# This is needed to save images \n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the train and val losses one line at a time.\n",
    "import re\n",
    "# regexes to find train and val losses on a line\n",
    "float_regex = r'[-+]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?'\n",
    "train_loss_re = re.compile('.*Train Loss: ({})'.format(float_regex))\n",
    "val_loss_re = re.compile('.*Val Loss: ({})'.format(float_regex))\n",
    "val_acc_re = re.compile('.*Val Acc: ({})'.format(float_regex))\n",
    "# extract one loss for each logged iteration\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "# NOTE: You may need to change this file name.\n",
    "with open('softmax.log', 'r') as f:\n",
    "    for line in f:\n",
    "        train_match = train_loss_re.match(line)\n",
    "        val_match = val_loss_re.match(line)\n",
    "        val_acc_match = val_acc_re.match(line)\n",
    "        if train_match:\n",
    "            train_losses.append(float(train_match.group(1)))\n",
    "        if val_match:\n",
    "            val_losses.append(float(val_match.group(1)))\n",
    "        if val_acc_match:\n",
    "            val_accs.append(float(val_acc_match.group(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.title('Softmax Learning Curve')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "fig.savefig('softmax_lossvstrain.png')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(val_accs, label='val')\n",
    "plt.title('Softmax Validation Accuracy During Training')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "fig.savefig('softmax_valaccuracy.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
